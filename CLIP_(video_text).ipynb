{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textebd18.pkl', 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Find the length of the longest audio embedding vector\n",
        "max_length = max(len(item['embeddings']) for item in data)\n",
        "\n",
        "# Pad the embedding vectors to the length of the longest vector\n",
        "for item in data:\n",
        "    embeddings = item['embeddings']\n",
        "    padding_length = max_length - len(embeddings)\n",
        "    padded_embeddings = np.pad(embeddings, (0, padding_length), mode='constant')\n",
        "    item['embeddings'] = padded_embeddings\n",
        "\n",
        "# Save the processed data to a new pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ],
      "metadata": {
        "id": "PwJIvQPcnBp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as file:\n",
        "    data = pickle.load(file)"
      ],
      "metadata": {
        "id": "18eSGA6Knzix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "buxgH4onn1Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in videos['embeddings']:\n",
        "    print(i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH0N6aYkoBhd",
        "outputId": "7d480d46-3404-4cf5-f147-cc280a25e889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(57200,)\n",
            "(57200,)\n",
            "(57200,)\n",
            "(57200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videoebd18.pickle', 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Find the length of the longest audio embedding vector\n",
        "max_length = 495360\n",
        "\n",
        "# Pad the embedding vectors to the length of the longest vector\n",
        "for item in data:\n",
        "    embeddings = item['embeddings']\n",
        "    padding_length = max_length - len(embeddings)\n",
        "    padded_embeddings = np.pad(embeddings, (0, padding_length), mode='constant')\n",
        "    item['embeddings'] = padded_embeddings\n",
        "\n",
        "# Save the processed data to a new pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ],
      "metadata": {
        "id": "mObV95YRnT-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksKkuVxgIr6n"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the embeddings from the pickle file\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "import pandas as pd\n",
        "textpkl = pd.DataFrame(text_data)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "videopkl = pd.DataFrame(video_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Ga9Vejhdkk",
        "outputId": "fcd7a04a-a198-40c5-ed1a-3f378d7f1b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'filename': 'assets-_1DDhUnyvwY.txt',\n",
              "  'embeddings': array([-0.09502907, -0.1150557 ,  0.2358259 , ...,  0.25622135,\n",
              "         -0.38683465, -0.13876839], dtype=float32)},\n",
              " {'filename': 'assets-_0F7wFPRBmY.txt',\n",
              "  'embeddings': array([-0.27596277, -0.51922   , -0.19767228, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32)},\n",
              " {'filename': 'assets-_1cHSTAv6aw.txt',\n",
              "  'embeddings': array([-0.48289317, -0.3653778 ,  0.19714946, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32)},\n",
              " {'filename': 'assets-__5k7e0f3r4.txt',\n",
              "  'embeddings': array([-0.9525771 ,  0.19983402,  0.39927948, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALIGNMENT**"
      ],
      "metadata": {
        "id": "FrXxYLCwiLYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(textpkl['embeddings'][0],\";\",videopkl['filename'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpaSPm4pRa3A",
        "outputId": "69d6c0b9-9506-49a3-e165-e46e78f26075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.09502907 -0.1150557   0.2358259  ...  0.25622135 -0.38683465\n",
            " -0.13876839] ; video-_0F7wFPRBmY.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textpkl['filename'][1],\";\",videopkl['embeddings'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05EflRm_g7pM",
        "outputId": "55645c3f-f0f2-4d82-fef7-5b116867688b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets-_0F7wFPRBmY.txt ; [ 1.9565936   1.1691937   0.33301967 ...  0.24661978  1.3313323\n",
            " -0.8962238 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textpkl['embeddings'][2],\";\",videopkl['embeddings'][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSzGJ6ACg8io",
        "outputId": "905cd540-cb18-47bd-c77d-593dda510c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.48289317 -0.3653778   0.19714946 ...  0.03739621 -0.674259\n",
            " -0.40728423] ; [-0.41057038  2.8972929   1.9146283  ... -0.3906505   1.564208\n",
            " -4.8234253 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textpkl['embeddings'][3],\";\",videopkl['embeddings'][3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqXde5bpg9Q2",
        "outputId": "10f2d584-017b-494a-d18c-3aeea3a165d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.9525771   0.19983402  0.39927948 ...  0.16337478 -0.04502395\n",
            " -0.29539204] ; [-1.1861308   0.53418887  0.04683519 ...  0.9691809   2.193101\n",
            " -0.7670548 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "UzYhcfO_eV0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "2ofiSFpkiqtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show clip"
      ],
      "metadata": {
        "id": "_EeqlFmThuUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    with open(os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename), 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Prepare the inputs\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "    text_input = preprocess(transcript).unsqueeze(0).to(device)\n",
        "\n",
        "    # Calculate features\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "id": "n5IsW6cEhp5t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "9e5cf92c-35ca-424b-eb97-b0f6ea7eac76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f1419ddf7bb5>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Prepare the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mvideo_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtext_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a photo of a \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Calculate features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected type {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'str'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    with open(os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename), 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Prepare the inputs\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "    text_input = preprocess(transcript).to(device)\n",
        "\n",
        "    # Calculate features\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "id": "Iaxx_EJgT7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    transcript_path = os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename)\n",
        "    with open(transcript_path, 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Prepare the inputs\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "    text_input = clip.tokenize(transcript).to(device)\n",
        "\n",
        "    # Calculate features\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "uwK_YzIWX9jd",
        "outputId": "89d57218-dfcb-4712-b3ca-178cdb8aea9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-18522e454e3a>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Prepare the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mvideo_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtext_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Calculate features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/clip.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meot_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input {texts[i]} is too long for context length {context_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input  Kids learning videos, presents dogs.\n A woman hugs the dog.\n A dog walks alongside a man in a field.\n A dog plays with its leash.\n Two dogs lie on the floor next to each other\n and playfully nip.\n Dog drinks water from dish.\n Talk lies on the floor, panting.\n Camera moves through grace. The image transitions from black and white to color.\n A dog runs through the grass at a park with its ears flapping.\n A dog jumps for tennis ball and catches it in its mouth.\n A dog chews on a tennis ball.\n A dog walks on a leash between two people.\n A dog lies across a woman's lap while she scratches it.\n A dog licks a cupcake.\n Three dogs chase a stick that is tossed into a lake.\n A dog swims in a lake with a tennis ball in its mouth.\n Kids learning videos, dog learning about dog facts for kids.\n is too long for context length 77"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    transcript_path = os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename)\n",
        "    with open(transcript_path, 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Split the transcript into chunks\n",
        "    max_tokens = model.context_length - 2  # Reserve 2 tokens for special tokens\n",
        "    transcript_chunks = [transcript[i:i + max_tokens] for i in range(0, len(transcript), max_tokens)]\n",
        "\n",
        "    # Calculate features for each transcript chunk\n",
        "    text_features = None\n",
        "    for chunk in transcript_chunks:\n",
        "        # Prepare the inputs\n",
        "        text_input = clip.tokenize([\"a photo of a \" + chunk]).to(device)\n",
        "\n",
        "        # Calculate features\n",
        "        with torch.no_grad():\n",
        "            chunk_features = model.encode_text(text_input)\n",
        "\n",
        "        # Concatenate the features\n",
        "        if text_features is None:\n",
        "            text_features = chunk_features\n",
        "        else:\n",
        "            text_features = torch.cat((text_features, chunk_features), dim=1)\n",
        "\n",
        "    # Prepare the video input\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "\n",
        "    # Calculate features for the video\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "id": "4cKrSu0cZBph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    transcript_path = os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename)\n",
        "    with open(transcript_path, 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Split the transcript into chunks\n",
        "    max_tokens = model.context_length - 2  # Reserve 2 tokens for special tokens\n",
        "    transcript_chunks = [transcript[i:i + max_tokens] for i in range(0, len(transcript), max_tokens)]\n",
        "\n",
        "    # Calculate features for each transcript chunk\n",
        "    text_features = None\n",
        "    for chunk in transcript_chunks:\n",
        "        # Prepare the inputs\n",
        "        text_input = clip.tokenize([\"a photo of a \" + chunk]).to(device)\n",
        "\n",
        "        # Calculate features\n",
        "        with torch.no_grad():\n",
        "            chunk_features = model.encode_text(text_input)\n",
        "\n",
        "        # Concatenate the features\n",
        "        if text_features is None:\n",
        "            text_features = chunk_features\n",
        "        else:\n",
        "            text_features = torch.cat((text_features, chunk_features), dim=1)\n",
        "\n",
        "    # Prepare the video input\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "\n",
        "    # Calculate features for the video\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "\n",
        "    # Reshape the text features\n",
        "    text_features = text_features.unsqueeze(0)\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.transpose(-1, -2)).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "id": "uGc8MeKiZf1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import clip\n",
        "\n",
        "# Load the video and text embeddings from pickle files\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/videopad18.pickle', 'rb') as f:\n",
        "    video_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/textpad18.pkl', 'rb') as f:\n",
        "    text_data = pickle.load(f)\n",
        "\n",
        "# Load the CLIP model and preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Prepare lists to store the video captions\n",
        "video_captions = []\n",
        "\n",
        "# Process video and text embeddings\n",
        "for video_item in video_data:\n",
        "    video_embedding = video_item['embeddings']\n",
        "    video_filename = video_item['filename']\n",
        "    video_suffix = video_filename[6:-4]  # Extract the suffix from video filename\n",
        "    text_filename = f\"assets-{video_suffix}.txt\"\n",
        "\n",
        "    # Load the transcript file for text data\n",
        "    transcript_path = os.path.join('/content/drive/MyDrive/Tvnv1/CLIP (video-text)/text18test', text_filename)\n",
        "    with open(transcript_path, 'r') as f:\n",
        "        transcript = f.read()\n",
        "\n",
        "    # Split the transcript into chunks\n",
        "    max_tokens = model.context_length - 2  # Reserve 2 tokens for special tokens\n",
        "    transcript_chunks = [transcript[i:i + max_tokens] for i in range(0, len(transcript), max_tokens)]\n",
        "\n",
        "    # Calculate features for each transcript chunk\n",
        "    text_features = None\n",
        "    for chunk in transcript_chunks:\n",
        "        # Prepare the inputs\n",
        "        text_input = clip.tokenize([\"a photo of a \" + chunk]).to(device)\n",
        "\n",
        "        # Calculate features\n",
        "        with torch.no_grad():\n",
        "            chunk_features = model.encode_text(text_input)\n",
        "\n",
        "        # Concatenate the features\n",
        "        if text_features is None:\n",
        "            text_features = chunk_features\n",
        "        else:\n",
        "            text_features = torch.cat((text_features, chunk_features), dim=1)\n",
        "\n",
        "    # Prepare the video input\n",
        "    video_input = torch.from_numpy(video_embedding).unsqueeze(0).to(device)\n",
        "\n",
        "    # Calculate features for the video\n",
        "    with torch.no_grad():\n",
        "        video_features = model.encode_image(video_input)\n",
        "\n",
        "    # Reshape the text features\n",
        "    text_features = text_features.view(1, -1, text_features.shape[-1])\n",
        "\n",
        "    # Compute similarity scores between video and text features\n",
        "    video_features /= video_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * video_features @ text_features.transpose(-1, -2)).softmax(dim=-1)\n",
        "\n",
        "    # Select the most relevant captions\n",
        "    values, indices = similarity[0].topk(5)\n",
        "    captions = []\n",
        "    for value, index in zip(values, indices):\n",
        "        caption = video_filename  # Replace with appropriate field for captions\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Generate video paragraph caption\n",
        "    video_captions.append(' '.join(captions))\n",
        "\n",
        "# Print the video paragraph captions\n",
        "for i, caption in enumerate(video_captions):\n",
        "    print(f\"Video {i+1}: {caption}\")"
      ],
      "metadata": {
        "id": "fptfvLB1Z6wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gbjaF0pbW_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}